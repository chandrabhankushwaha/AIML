{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e01d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (0.1.72)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_lg-0.5.0.tar.gz"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Shilpi\\\\anaconda3\\\\Lib\\\\site-packages\\\\~3acy\\\\attrs.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_lg-0.5.0.tar.gz (532.3 MB)\n",
      "Collecting spacy<3.3.0,>=3.2.3\n",
      "  Using cached spacy-3.2.4-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (21.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (61.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (1.0.9)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Using cached thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (0.7.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (1.21.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (4.64.0)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Using cached srsly-2.4.4-cp39-cp39-win_amd64.whl (450 kB)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (3.0.8)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (8.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (1.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.3->en-core-sci-lg==0.5.0) (2.0.1)\n",
      "Installing collected packages: catalogue, srsly, thinc, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.1\n",
      "    Uninstalling catalogue-1.0.1:\n",
      "      Successfully uninstalled catalogue-1.0.1\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.5\n",
      "    Uninstalling thinc-7.4.5:\n",
      "      Successfully uninstalled thinc-7.4.5\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.7\n",
      "    Uninstalling spacy-2.3.7:\n",
      "      Successfully uninstalled spacy-2.3.7\n",
      "Requirement already satisfied: spacy in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (3.2.4)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.4.1-cp39-cp39-win_amd64.whl (11.8 MB)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Using cached thinc-8.1.4-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.17\n",
      "    Uninstalling thinc-8.0.17:\n",
      "      Successfully uninstalled thinc-8.0.17\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.4\n",
      "    Uninstalling spacy-3.2.4:\n",
      "      Successfully uninstalled spacy-3.2.4\n",
      "Successfully installed spacy-3.4.1 thinc-8.1.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.4.1 which is incompatible.\n",
      "en-core-sci-lg 0.5.0 requires spacy<3.3.0,>=3.2.3, but you have spacy 3.4.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (1.7.3)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (61.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (1.0.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apyori in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (1.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yellowbrick in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (1.5)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shilpi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: 'c:\\\\users\\\\shilpi\\\\anaconda3\\\\lib\\\\site-packages\\\\pybind11-2.10.0.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "# IMPORT NECESSARY BUILT IN LIBRARIES\n",
    "!pip install contractions\n",
    "!pip install tqdm\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_lg-0.5.0.tar.gz\n",
    "!pip install -U spacy\n",
    "!pip install mlxtend\n",
    "!pip install apyori\n",
    "!pip install yellowbrick\n",
    "!pip install texthero -q\n",
    "!pip install textblob\n",
    "!python -m spacy download en_core_web_sm\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40178c5d",
   "metadata": {},
   "source": [
    "# Read the data and add header to column, Since we need ton consider only titles. Lets give header name as 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CORD19-10k.csv\", \n",
    "                  sep='\\t', \n",
    "                  names=[\"Title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766a6f4",
   "metadata": {},
   "source": [
    "# Lets see some data from title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above we can see we have a column in proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cdbbc",
   "metadata": {},
   "source": [
    "# Lets see size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above we see we have 1000 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c020f",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find the lenght of the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43396acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length of message\"] = df[\"Title\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bc097",
   "metadata": {},
   "source": [
    "# Expanding Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1660e48",
   "metadata": {},
   "source": [
    "Contactions ate those little literary shortcuts we take where instead of \"Should haave\" we prefer \"Should've\" or where \"Do not\" quickly become Don't. We will add new coloumn to our dataframe called \"Title_no_contract\" aand apply laambda function to the \"Title\" filed which will expand any contactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d96774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_with_no_contract']=df['Title'].apply(lambda x:[contractions.fix(word) for word in x.split()])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079ea3a",
   "metadata": {},
   "source": [
    "Convert title_with_no_contract coloumn into string to proceed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_with_no_contract']=df['title_with_no_contract'].astype('string')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625f865",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "tokenization will split each individual word into a token, we will apply NLTK.word_tokenize() function and create aa new columns names ='tokenized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ad223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokenized']=df['title_with_no_contract'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc10fb",
   "metadata": {},
   "source": [
    "# Converting all characters to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac25a7d",
   "metadata": {},
   "source": [
    "Tarnsforming all words to lowercase is very common pre processing step. We will append a new coloumn names \"lower\" to the dataframe which will transfom all the tokenized words into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_in_lowercase']=df['title_tokenized'].apply(lambda x:[word.lower() for word in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ce410",
   "metadata": {},
   "source": [
    "# Removing punctuations - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95638a5e",
   "metadata": {},
   "source": [
    "it is often removed from our data since they serve little value to the data. we will create a new coloumn which a the puntuation removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=string.punctuation\n",
    "df['title_with_no_punc']=df['title_in_lowercase'].apply(lambda x:[word for word in x if word not in punc])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda16aae",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b3fa6",
   "metadata": {},
   "source": [
    "We ultimately would want the expanded contractions to be tokenized seperaately. There fore lets convert the lists under the \"Title_no_contract\" column back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "df['title_with_stopwords_removed']=df['title_with_no_punc'].apply(lambda x:[word for word in x if word not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bfd8d",
   "metadata": {},
   "source": [
    "# Finally we get clean data after various preprocessing which we will use further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6749d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Data']=df['title_with_stopwords_removed'].astype('string')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ded15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c56c2026",
   "metadata": {},
   "source": [
    "# Lets drop null values if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d496c",
   "metadata": {},
   "source": [
    "# From above we can see there is no null vaues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2bd7d",
   "metadata": {},
   "source": [
    "# Convert titles into corresponding feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00668365",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = df['Clean_Data'].values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767d250",
   "metadata": {},
   "source": [
    "td_arr=X.toarray()\n",
    "print(len(td_arr))\n",
    "td_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f922f14",
   "metadata": {},
   "source": [
    "df_in= pd.DataFrame(td_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9fbbd",
   "metadata": {},
   "source": [
    "df_in.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb54fe1",
   "metadata": {},
   "source": [
    "df_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa199baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "print(len(word_features))\n",
    "print(word_features[5000:5100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020af8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(len(word_features2))\n",
    "print(word_features2[:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()\n",
    "X3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e183239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b8e46df",
   "metadata": {},
   "source": [
    "#  From above we got  feature vectors which is a sparse matrix with 64948 stored elements in Compressed Sparse Row format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c6c7d",
   "metadata": {},
   "source": [
    "# Converting title text into corresponding TF-IDF vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4ff1f",
   "metadata": {},
   "source": [
    "Lets Calculate Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb468d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    tf1 = (df['Title'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "    tf1.columns = ['words','tf']\n",
    "    return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (df['Title'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcdadc",
   "metadata": {},
   "source": [
    "# Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d890065",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (df['Title'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(df[df['Title'].str.contains(word)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above we can see TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c87e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "610f0552",
   "metadata": {},
   "source": [
    "# PCA - Reducing sparse matrix into 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sparse matrix we can either use SparsePCA or TruncatedSVD for sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "transformer = SparsePCA(n_components=2, random_state=0)\n",
    "transformer.fit(X3.toarray())\n",
    "X_transformed = transformer.transform(X3.toarray())\n",
    "X_transformed\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db96a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD \n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "data = svd.fit_transform(X3.toarray()) \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fe326",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction for 2D Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80abc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\n",
    "X_embedded = tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7829c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad823da",
   "metadata": {},
   "source": [
    "# 2. Perform K-means clustering for different values of K (2,3,4,5) and evaluate the quality of clustering using Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc00c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing k mean clustering lets find the optimul number of cluster by using elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ea168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    kmeans.fit(X3)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.savefig('elbow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250fb18",
   "metadata": {},
   "source": [
    "# From above it seems 2 number of cluster will be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222aa86",
   "metadata": {},
   "source": [
    "# K Mean clustering for K=2 and evaaluating quality using Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, init = \"k-means++\", random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X3)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Silhouette Score(n=2): {silhouette_score(data, y_kmeans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3f227",
   "metadata": {},
   "source": [
    "# K Mean clustering for K=3 and evaaluating quality using Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e644ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, init = \"k-means++\", random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X3)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a17219",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Silhouette Score(n=3): {silhouette_score(X3, y_kmeans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10884cc",
   "metadata": {},
   "source": [
    "# K Mean clustering for K=4 and evaaluating quality using Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f69cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4, init = \"k-means++\", random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X3)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Silhouette Score(n=4): {silhouette_score(X3, y_kmeans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da73e23",
   "metadata": {},
   "source": [
    "# K Mean clustering for K=5 and evaaluating quality using Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c590fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4, init = \"k-means++\", random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X3)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5034dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Silhouette Score(n=4): {silhouette_score(X3, y_kmeans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030dd5e",
   "metadata": {},
   "source": [
    "# For each value of K, plot clusters (all cluster points in same cluster with same color, cluster points in different cluster in different colors). [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da6670",
   "metadata": {},
   "source": [
    "# For K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[y_kmeans==0, 0], data[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(data[y_kmeans==1, 0], data[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc14e11",
   "metadata": {},
   "source": [
    "# For K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[y_kmeans==0, 0], data[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(data[y_kmeans==1, 0], data[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(data[y_kmeans==2, 0], data[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c716d7",
   "metadata": {},
   "source": [
    "# For K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[y_kmeans==0, 0], data[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(data[y_kmeans==1, 0], data[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(data[y_kmeans==2, 0], data[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "plt.scatter(data[y_kmeans==3, 0], data[y_kmeans==3, 1], s=100, c='cyan', label ='Cluster 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025192c",
   "metadata": {},
   "source": [
    "# For K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[y_kmeans==0, 0], data[y_kmeans==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(data[y_kmeans==1, 0], data[y_kmeans==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(data[y_kmeans==2, 0], data[y_kmeans==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "plt.scatter(data[y_kmeans==3, 0], data[y_kmeans==3, 1], s=100, c='cyan', label ='Cluster 4')\n",
    "plt.scatter(data[y_kmeans==4, 0], data[y_kmeans==4, 1], s=100, c='magenta', label ='Cluster 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4806ec1",
   "metadata": {},
   "source": [
    "# b) Draw a bar graph with X-axis as K value and Y-axis as silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67de936",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of_clusters=[2,3,4,5,6,7,8,9,10]\n",
    "arr_sil=[]\n",
    "for i_count in range_of_clusters:\n",
    "    km=KMeans(n_clusters=i_count)\n",
    "    km.fit(X3)\n",
    "    cluster_labels= km.fit_predict(X_transformed)\n",
    "    silhouette_avg = silhouette_score(X3, cluster_labels)\n",
    "    print(\"For N_clusters = \",i_count,\"The average silhouette_score is :\",silhouette_avg)\n",
    "    arr_sil.append(silhouette_avg)\n",
    "plt.plot(range_of_clusters,arr_sil, 'bx-')\n",
    "plt.bar(range_of_clusters,arr_sil)\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title('Silhouette analysis For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791968d1",
   "metadata": {},
   "source": [
    "# 2 c)  Explain (1-2 lines) which is the best clustering and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "for i in [2, 3, 4, 5]:\n",
    "    '''\n",
    "    Create KMeans instance for different number of clusters\n",
    "    '''\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "    q, mod = divmod(i, 2)\n",
    "    '''\n",
    "    Create SilhouetteVisualizer instance with KMeans instance\n",
    "    Fit the visualizer\n",
    "    '''\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c5a9d",
   "metadata": {},
   "source": [
    "# We can say that the clusters are well apart from each other as the silhouette score is closer to 1\n",
    "# From above graph and score we can say that the optimal number of clusters is 3 as its silhouette score is greater than that of other 3 clusters.\n",
    "# Silhouette Score(n=3 or n=3): 0.018937457053013588\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49b18c",
   "metadata": {},
   "source": [
    "# 3. Consider the per-processed title text of all articles and find words which frequently occur together. [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5d450",
   "metadata": {},
   "source": [
    "# We will find ngrams and Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def combination_of_words(df):\n",
    "    return (TextBlob(df['Title'][0]).ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_of_words(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caf4a8",
   "metadata": {},
   "source": [
    "# Above 2 words are frequently occur together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a513852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "df['Body-Collocation'] = df.apply(lambda df: BigramCollocationFinder.from_words(df['Title']),axis=1)\n",
    "df['Body-Collocation'] = df['Body-Collocation'].apply(lambda df: df.nbest(bigram_measures.pmi, 3))\n",
    "print (df['Body-Collocation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "bigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "df['Body-Collocation'] = df.apply(lambda df: TrigramCollocationFinder.from_words(df['Title']),axis=1)\n",
    "df['Body-Collocation'] = df['Body-Collocation'].apply(lambda df: df.nbest(bigram_measures.pmi, 3))\n",
    "print (df['Body-Collocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664dd4",
   "metadata": {},
   "source": [
    "# 3 a. find association rules of minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(df[\"Clean_Data\"].apply(lambda x:x.split(\",\") ))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8bc86",
   "metadata": {},
   "source": [
    "Apriori Algorithm and One-Hot Encoding\n",
    "\n",
    "Apriori's algorithm transforms True/False or 1/0.\n",
    "Using TransactionEncoder, we convert the list to a One-Hot Encoded Boolean list.\n",
    "Products that customers bought or did not buy during shopping will now be represented by values 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TransactionEncoder()\n",
    "a_data = a.fit(data).transform(data)\n",
    "df = pd.DataFrame(a_data,columns=a.columns_)\n",
    "df = df.replace(False,0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eacd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    " \n",
    "frequent_items = apriori(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "rules = association_rules(frequent_items, metric=\"lift\", min_threshold=1)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0aee19",
   "metadata": {},
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "rules = association_rules(df, min_support = 0.001, max_length=2)\n",
    "print('Execution time: ', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b59b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "supps = rules[\"support\"].values\n",
    "sns.distplot(supps[supps > 0], kde=False, bins=np.arange(0, max(supps), 0.2))\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Number of Rules\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatterplot using support and confidence\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x = \"support\", y = \"confidence\", \n",
    "                size = \"lift\", data = rules)\n",
    "plt.margins(0.01,0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2424fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the style\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.style.use('seaborn-white')\n",
    "#Plotting the relationship between the metrics\n",
    "plt.subplot(221)\n",
    "sns.scatterplot(x=\"support\", y=\"confidence\",data=rules)\n",
    "plt.subplot(222)\n",
    "sns.scatterplot(x=\"support\", y=\"lift\",data=rules)\n",
    "plt.subplot(223)\n",
    "sns.scatterplot(x=\"confidence\", y=\"lift\",data=rules)\n",
    "plt.subplot(224)\n",
    "sns.scatterplot(x=\"antecedent support\", y=\"consequent support\",data=rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert rules to coordinates.\n",
    "def rules_to_coordinates(rules):\n",
    "    rules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\n",
    "    rules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\n",
    "    rules['rule'] = rules.index\n",
    "    return rules[['antecedent','consequent','rule']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Compute the frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support = 0.20, \n",
    "                            use_colnames = True, max_len = 2)\n",
    "\n",
    "# Compute rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric = 'confidence', \n",
    "                          min_threshold = 0.55)\n",
    "\n",
    "# Convert rules into coordinates suitable for use in a parallel coordinates plot\n",
    "coords = rules_to_coordinates(rules)\n",
    "\n",
    "# Generate parallel coordinates plot\n",
    "plt.figure(figsize=(4,8))\n",
    "parallel_coordinates(coords, 'rule')\n",
    "plt.legend([])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f120cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41af23",
   "metadata": {},
   "source": [
    "#  association rules  10 % support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1124e",
   "metadata": {},
   "source": [
    "df = apriori(df, min_support = 0.1, use_colnames = False, verbose = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f0d15",
   "metadata": {},
   "source": [
    "#  association rules  20% support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a threshold value for the support value and calculate the support value.\n",
    "df = apriori(df, min_support = 0.2, use_colnames = True, verbose = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f46138",
   "metadata": {},
   "source": [
    "#  association rules  20% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's view our interpretation values using the Associan rule function.\n",
    "df_ar = association_rules(df, metric = \"confidence\", min_threshold = 0.2)\n",
    "df_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c075fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the style\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.style.use('seaborn-white')\n",
    "#Plotting the relationship between the metrics\n",
    "plt.subplot(221)\n",
    "sns.scatterplot(x=\"support\", y=\"confidence\",data=rules)\n",
    "plt.subplot(222)\n",
    "sns.scatterplot(x=\"support\", y=\"lift\",data=rules)\n",
    "plt.subplot(223)\n",
    "sns.scatterplot(x=\"confidence\", y=\"lift\",data=rules)\n",
    "plt.subplot(224)\n",
    "sns.scatterplot(x=\"antecedent support\", y=\"consequent support\",data=rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
